{
 "cells": [
  {
   "cell_type": "raw",
   "id": "5b7e5db9",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"Hamel Husain Eval Notes\"\n",
    "author: \"Jaume Amores\"\n",
    "format:\n",
    "  revealjs:\n",
    "    scrollable: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ecedcd",
   "metadata": {},
   "source": [
    "# LLM Evals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa4c9e0",
   "metadata": {},
   "source": [
    "# Your AI Product Needs Evals\n",
    "\n",
    "[web site](https://hamel.dev/blog/posts/evals/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb1e72e",
   "metadata": {},
   "source": [
    "## Iterating Quickly == Success\n",
    "\n",
    "You must have tools and processes for:\n",
    "\n",
    "- Evaluating quality (Testing)\n",
    "- Debugging  (logging and inspecting data) \n",
    "- Changing behaviour (prompt engineering, fine tuning, Writing code)\n",
    "\n",
    "Recommended processes:\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    A --> B\n",
    "    A --> C\n",
    "    B --> D\n",
    "    C --> D\n",
    "    D --> E\n",
    "    D --> F\n",
    "    E --> G\n",
    "    F --> G\n",
    "    D -.-> D_note[\"Human review\\nModel-based\\nA/B tests\"]\n",
    "\n",
    "    A[\"LLM\\nInvocation\\n(synthetic/human inputs)\"]\n",
    "    B[\"Unit\\ntests\"]\n",
    "    C[\"Logging\\nTraces\"]\n",
    "    D[\"Eval &\\nCuration\"]\n",
    "    E[\"Fine-tuning\"]\n",
    "    F[\"Prompt Eng.\"]\n",
    "    G[\"Improve\\nModel\"]\n",
    "    \n",
    "\n",
    "    style D fill:#FFD966,stroke:#333,stroke-width:2px,color:#000\n",
    "    style D_note fill:none,stroke:none,color:#FFFFFF\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03db79ae",
   "metadata": {},
   "source": [
    "## Case study\n",
    "\n",
    "- First stages of fast improvement due to prompt. engineering.\n",
    "- Then stuck due to improvement in one place leading to failures in others.\n",
    "\n",
    "## Evals\n",
    "\n",
    "- Cost of A/B testing > cost of model-based and human evals > cost of unit tests\n",
    "- Cadence: unit-tests after each code change, model-based + human evals with some cadence, A/B tests after major changes.\n",
    "\n",
    "## Unit tests\n",
    "\n",
    "- Assertions like in pytest\n",
    "- In more places: data cleaning and automatic retries (using assertions to course-correct) during inference.\n",
    "- Should be fast to run often.\n",
    "- To come up with unit tests:\n",
    "    - Think about your traces and the failure modes they incur.\n",
    "    - Ask your model to brainstorm.\n",
    "- Step 1: write scoped tests\n",
    "    - Break down the scope into features and scenarios\n",
    "    - For example, one feature of Lucy is to find real estate listings, for example: \"Find listings with more than 3 bedrooms and less than $2M in San Jose, CA\"\n",
    "    - The assertion verifies that the expected **number of results** is returned, scenarios: \n",
    "        - only one listing, more than one listing, no listing.\n",
    "    - Generic tests: do not include UUID of user in response.\n",
    "- Step 2: create test cases:\n",
    "    - Inputs that trigger each of the scenarios.\n",
    "    - Use synthetic inputs based on LLM. \n",
    "    - If possible, write both instructions for obtaining the response as well as instructions for verifying the result. \n",
    "        - For example \"write 50 different instructions that a real estate agent can give to his assistant to create contacts on his CRM. For each of the instructions, generate a second instruction to look up the created contact\". \n",
    "        - For each of the test cases, we execute the first user input to create the contact and then execute the second to fetch the contact. If the result length is not exactly 1, the test fails.\n",
    "    - One signal the tests are good is when the model struggles to pass them.\n",
    "    - You don't need 100% pass rate.\n",
    "- Step 3: run and track your tests regularly.\n",
    "\n",
    "## Level 2: Human & Model Eval\n",
    "\n",
    "- Logging traces\n",
    "    - For example, LangSmith\n",
    "- Looking at traces\n",
    "    - Remove all friction from the process of looking at data. \n",
    "        - Build your own data viewing and labelling tool => Shiny for python.\n",
    "        - Filter by scenario or feature, go to trace, check if input is human or synthetic, ...\n",
    "    - Make the output of the LLM editable.\n",
    "    - Lilac: \n",
    "        - Search and filter data semantically.\n",
    "        - Find a set of similar data points while debugging an issue\n",
    "- How much data:\n",
    "    - At least read traces for all test cases and all user-generated traces. Sample over time.\n",
    "- Automated Evaluation with LLMs\n",
    "    - Have humans periodically evaluate a sample of traces. \n",
    "        - Track correlation between human and model evaluations.\n",
    "    - Collect \"critiques\" from labelers explainig why they are making a decision. \n",
    "        - Use them for prompt engineering and fine tuning of the LLM evaluator.\n",
    "    - Use the most powerful model you can afford.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2386c7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Using LLM-as-a-Judge For Evaluation: A Complete Guide\n",
    "\n",
    "[blog](https://hamel.dev/blog/posts/llm-judge/#the-problem-ai-teams-are-drowning-in-data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83b8dba",
   "metadata": {},
   "source": [
    "## Step 1: Find the principal domain expert.\n",
    "\n",
    "- Get one principal domain expert evaluate LLM output\n",
    "- Use binary decisions.\n",
    "- Include critique.\n",
    "\n",
    "## Step 2: Create dataset\n",
    "\n",
    "- Diverse: define problem in terms of *dimensions* and have inputs for each combination.\n",
    "    - Example of dimensions: features, scenarios, personas\n",
    "    - One input per each combination of feature, scenario and persona.\n",
    "- Types:\n",
    "    - logged real interactions\n",
    "    - synthetic\n",
    "- Use real DB and APIs to get the data so it is a realistic as possible.\n",
    "\n",
    "## Step 3: Evaluate accuracy on created dataset\n",
    "\n",
    "- Remove friction for domain exper to evaluate.\n",
    "    - May need to get additional context: metadata about the user, state of current system (time, inventory levels ...), resources to check => ability to check a database.\n",
    "    - All this into single page\n",
    "    - Build simple web app to review data => Shiny for python.\n",
    "\n",
    "- How much data:\n",
    "    - 30 examples and keep going until no more failures. Then keep going until I don't learn anything new.\n",
    "\n",
    "## Step 4: Fix Errors\n",
    "\n",
    "- Pervasive errors? (or failures?)\n",
    "\n",
    "## Step 5: Build LLM as judge\n",
    "\n",
    "- Spreadsheet with:\n",
    "    - model response\n",
    "    - jugdge critique\n",
    "    - jugdge decision\n",
    "    - Expert critique\n",
    "    - Expert decision\n",
    "    - Expert revised response (what the model should have outputted)\n",
    "    - Agreement between judge and expert (true / false)\n",
    "\n",
    "- Sometimes we need precision / recall instead of agreement if the dataset is imbalanced (more failures than passes, or the other way)\n",
    "\n",
    "- Iterate using better prompts (with expert's critiques as new examples?) until > 90% accuracy / F1 / ...\n",
    "- Adjust prompts by hand or using ALIGN Eval\n",
    "- What if this doesnâ€™t work?\n",
    "    - We may need to rely more on human annotations.\n",
    "- Mistakes in LLM judges due to:\n",
    "    - Not providing critiques, or providing very terse critiques.\n",
    "    - Not providing enough context. Everything used to evaluate the quality of the judge should be also given to it as context.\n",
    "    - Not providing diverse examples.\n",
    "\n",
    "## Step 6: Error Analysis\n",
    "\n",
    "- Apply judge against real or synthetic interactions, always on unseen data.\n",
    "- Measure error rate on each segment of data, i.e., combination of feature, scenario, and persona in our example.\n",
    "- Look at each type of error and classify it by hand, after looking at the whole trace (including tool calls made and what context / insight was extracted from each) for example: \n",
    "    - Missing user Education.\n",
    "    - Authentication issues.\n",
    "    - Poor Context Handling.\n",
    "    - Inadequate Error Messages.\n",
    "- Fix Errors again.\n",
    "    - Go back to step 3 and iterate until satisfied.\n",
    "    - Try to write a test case for the error.\n",
    "- Data Literacy and statistics [link](https://jxnl.co/writing/2024/06/02/10-ways-to-be-data-illiterate-and-how-to-avoid-them/)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86c8c39",
   "metadata": {},
   "source": [
    "## Step 7: Create More Specialized LLM Judges, if needed\n",
    "\n",
    "- For example, if the judge is poor at citing sources correctly, we can create a targeted eval for that, or even use code-based assertions without judge.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67cf651",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# A Field Guide to Rapidly Improving AI Products\n",
    "\n",
    "https://hamel.dev/blog/posts/field-guide/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a7eec6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Error Analysis\n",
    "\n",
    "Example of success:\n",
    "- Team built a simple viewer to examine conversations. \n",
    "- Next to each conversation was a space for open-ended notes about failure modes.\n",
    "- After annotating dozens of conversations, clear patterns emerged. \n",
    "    - For instance, their model was struggling with date handling failing 66% of the time.\n",
    "- Real case example:\n",
    "    - See what things the users are asking for and how well the model satisfies their needs in each case. This makes building the road map without effort.\n",
    "    - See how people assumed your product would work.\n",
    "    - By looking at how the model responds in each case you start to be able to predict where it will fail and how to improve it via RAG, Prompt Engineering, etc.\n",
    "    - Custom viewer that has button for categorizing failures.\n",
    "    - Brain Trust: automate implementation of unit tests or other eval techniques to measure how the changes made help improve those failure modes.\n",
    "- Summary. The process of error analysis consists of:\n",
    "    - Looking at the conversations.\n",
    "    - Writing detail notes about how each conversation failed.\n",
    "    - Categorizing the notes (or only the failures)\n",
    "    - The latter can sometimes be made semi-automatic by using an LLM to classify those notes.\n",
    "\n",
    "## Custom Data Viewer\n",
    "\n",
    "- Each use case has its specificities that are rarely covered by off-the-shelf tools.\n",
    "- Even small UX decisions make the difference between the team using the tool or not.\n",
    "- What makes a good data viewer:\n",
    "    - Show all context in one place. No need to switch.\n",
    "    - Make feedback trivial to capture. A simple button.\n",
    "    - Capture open-ended feedback.\n",
    "    - Enable quick filtering and sorting. Make it easy to dive into specific error types.\n",
    "    - Have hotkeys. \n",
    "\n",
    "## Empower Domain Experts\n",
    "\n",
    "- Give domain expertrs tools to write and iterate on prompts directly.\n",
    "- Prompt playgrounds like LangSmith and Braintrust are good for this.\n",
    "- Integrated prompt environments: admin versions of their actual user interface that expose prompt editing.\n",
    "- Avoid technical jargon when talking to domain experts.\n",
    "\n",
    "## Generate syntethic data\n",
    "\n",
    "- Choose right dimensions to test. Example with Real Estate product:\n",
    "    - Features: different capabilities of the product, Examnpl\n",
    "        - find listings matching criteria\n",
    "        - analyze trends and princing\n",
    "        - setting up property viewings\n",
    "        - post-viewing communication\n",
    "    - Scenarios: different situations in which the product is used.\n",
    "        - Exact match\n",
    "        - Multiple matches\n",
    "        - No matches\n",
    "        - Invalid criteria\n",
    "    - Personas: different types of users:\n",
    "        - first-time homebuyer\n",
    "        - property investor\n",
    "        - luxury home seeker\n",
    "        - relocating family\n",
    "- Ensure synthetic data triggers the dimensions to be tested:\n",
    "    - Test database with enough variety to cover all dimensions.\n",
    "        - This can be anonymized production data.\n",
    "    - A way to verify that the generated queries actually trigger the intended dimensions.\n",
    "- It is key that synthetic data is grounded in real system constraints:\n",
    "    - real listings, real agent schedules, restricting business rules, including local regulations, etc.\n",
    "- If we don't have production data because the product is new, use LLMs to generate both test queries and test data.\n",
    "    - Use realistic attributes \n",
    "        - prices that match market conditions, valid addresses with real street names, etc.\n",
    "- Guidelines for using synthetic data:\n",
    "    - Diserify dataset: cover all dimensions.\n",
    "    - Generate user inputs, not outputs: realistic user queries, not LLM responses.\n",
    "    - Incorporate real system constraints: use real data and business rules.\n",
    "    - Verify dimension coverage: ensure generated queries trigger intended dimensions.\n",
    "    - Start simple then add complexity: begin with basic queries, then introduce edge cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09818be",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Keep trust in Eval system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725c5a68",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Criteria drift \n",
    "\n",
    "- Evaluation criteria evolve as you observe more model outputs.\n",
    "    - The process of reviewing AI outputs helps articulate our own evaluation standards.\n",
    "    - We need to treat evaluation criteria as living documents that evolve with our understanding.\n",
    "    - Different stakeholders may have different criteria and we need to reconcile them rather than imposing a single standard.\n",
    "\n",
    "### Trustworthy evaluation systems\n",
    "\n",
    "- How: \n",
    "    - As discussed: binary metrics + critiques, and measuring alignment with human judgements.\n",
    "    - And scaling correctly:\n",
    "        - start with high human involvement\n",
    "        - study alignment patterns and focus manual evaluation on areas of disagreement\n",
    "        - use strategic sampling: \n",
    "            - sample outputs that provide more information.\n",
    "            - more weight on areas of disagreement.\n",
    "        - keep regular calibration as you scale.\n",
    "- Scaling is not about reducing human effort but redirecting it towards the most impactful areas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a247de",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Plan experiments not features\n",
    "\n",
    "To complete reading."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08811e45",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Further reading\n",
    "\n",
    "- [ALIGN Eval](https://eugeneyan.com/writing/aligneval/)\n",
    "    - [blog post](https://eugeneyan.com/writing/aligneval/)\n",
    "- [LLM as judges](https://hamel.dev/blog/posts/llm-judge/index.html#resources):\n",
    "    - [survey on LLM as judge approaches](https://eugeneyan.com/writing/llm-evaluators/)\n",
    "    - [similar approach](https://www.databricks.com/blog/enhancing-llm-as-a-judge-with-grading-notes)\n",
    "    - [end-to-end example](https://cookbook.openai.com/examples/custom-llm-as-a-judge)\n",
    "    - [DOSU](https://blog.langchain.dev/dosu-langsmith-no-prompt-eng/)\n",
    "- [OpenAI cookbook](https://cookbook.openai.com/examples/partners/eval_driven_system_design/receipt_inspection)\n",
    "- [G-Eval](https://deepeval.com/docs/metrics-llm-evals)\n",
    "    - [G-Eval Simply Explained: LLM-as-a-Judge for LLM Evaluation](https://www.confident-ai.com/blog/g-eval-the-definitive-guide)\n",
    "- [Inspect](https://www.youtube.com/watch?v=kNaZU9bz-UM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33827e4c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Other Links\n",
    "\n",
    "- [fine-tuning](https://parlance-labs.com/education/#fine-tuning)\n",
    "- [evaluating RAGs](https://jxnl.github.io/blog/writing/2024/02/28/levels-of-complexity-rag-applications/)\n",
    "    - [6 RAG Evals](https://jxnl.co/writing/2025/05/19/there-are-only-6-rag-evals/)\n",
    "    - [RAG is dead](https://pashpashpash.substack.com/p/why-i-no-longer-recommend-rag-for)\n",
    "    - [RAG is not dead](https://hamel.dev/notes/llm/rag/not_dead.html)\n",
    "- [A/B testing](https://www.geteppo.com/blog)\n",
    "- [Continual In-Context Learning](https://blog.langchain.dev/dosu-langsmith-no-prompt-eng/)\n",
    "- [prompt caching](https://platform.openai.com/docs/guides/prompt-caching)\n",
    "- [error analysis](https://youtu.be/JoAxZsdw_3w)\n",
    "    - [Hamel recap and blog collection on error analysis](https://hamel.dev/blog/posts/evals-faq/#q-why-is-error-analysis-so-important-in-llm-evals-and-how-is-it-performed)\n",
    "    - [Hamel's video walkthrough](https://youtu.be/qH1dZ8JLLdU)\n",
    "- [data literacy](https://jxnl.co/writing/2024/06/02/10-ways-to-be-data-illiterate-and-how-to-avoid-them/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3a6125",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
