{
 "cells": [
  {
   "cell_type": "raw",
   "id": "3198b7b7",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"RAG talks in Parlance\"\n",
    "author: \"Jaume Amores\"\n",
    "format:\n",
    "  revealjs:\n",
    "    scrollable: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfa603a",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Back to Basics for RAG\n",
    "\n",
    "[video](https://parlance-labs.com/education/rag/jo.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5551ede",
   "metadata": {},
   "source": [
    "## Demistifying RAG\n",
    "\n",
    "- It is basically \"stuffing text into the LLM prompt\".\n",
    "    - Typical use case: Q&A or search:\n",
    "        - Ask open-ended question.\n",
    "        - Retrieve content related to this question\n",
    "        - Use this content as context by stuffing it into the LLM prompt\n",
    "        - Result: the LLM response will be \"grounded\" in this context.\n",
    "        - It is not hallucination free but it might improve the accuracy of the generated answer.\n",
    "\n",
    "- Not necessarily related with Q&A or search.\n",
    "    - Example: labeller, retrieve positive and negative examples from dataset and present them in prompt to have the LLM label remaining following those examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ff070c",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "\n",
    "- Orchestration\n",
    "- Evaluation\n",
    "- Prompt\n",
    "- LLM\n",
    "- State (retrieval sources):\n",
    "    - File, Search Engine, Vector Database, Database, Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d18f290",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Evaluating Information Retrieval Systems\n",
    "\n",
    "- Query\n",
    "- Retrieval process\n",
    "- Result: ranked list of documents\n",
    "- Evaluation: relevance of ranking wrt query.\n",
    "    - Metrics: binary, non-binary\n",
    "    - Benchmarks: TREC, MS Marco, BEIR, ...\n",
    "    - Critical: build your own relevance dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5813f2f",
   "metadata": {},
   "source": [
    "## Relevance dataset\n",
    "\n",
    "- If we have real queries from production, spend time labelling results of those queries.\n",
    "- Otherwise, ask LLM to generate realistic synthetic queries.\n",
    "- Ideally static collection: no additions while evaluating and comparing, to keep consistency.\n",
    "- Using LLM judges to evaluate result.\n",
    "    - Need to find appropriate prompt to make it correlate with human judgement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b783c183",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Representational approaches\n",
    "\n",
    "- Avoid scoring all documents (e.g., if we use at web scale)\n",
    "- Have indexed documents, score only subset.\n",
    "- Approaches:\n",
    "    - Sparse, using inverted index.\n",
    "        - Top-k retrieval algorithms: WAND, ...\n",
    "        - Supervised (splade) or unsupervised (tf-idf)\n",
    "    - Dense: \n",
    "        - Vector index\n",
    "        - Accelerated search (approximate)\n",
    "        - Supervised via transf-learning (text embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0fdc59",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Dense representations\n",
    "\n",
    "- Encoder / Transformer style:\n",
    "    - Tokenize input text into discrete vocabulary.\n",
    "    - Pretrained learned representations of each token.\n",
    "    - Feed into encoder. For each token get an output vector.\n",
    "    - Pooling stage: average all vectors into a single vector representation. \n",
    "        - Weakest aspect. Diluted, low precision. Need shrinking mechanism?\n",
    "\n",
    "- Baseline / benchmark: BM25\n",
    "    - Can avoid spectacular failures, e.g., those caused by out-of-vocabulary problems.\n",
    "    - BM25 can also act as a strong baseline, e.g., for long-context use case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2072853d",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Hybrid approaches\n",
    "\n",
    "- Dense + Sparse representations: overcome fixed vocabulary issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c80f49",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Chunking\n",
    "\n",
    "- Dense representations should not use texts with more than 256 tokens for high precision search (maybe it's ok for other applications like classification)\n",
    "    - Because they haven't been trained on them.\n",
    "    - Topic drifts with longer texts.\n",
    "- You need to chunk if longer than 256 tokens\n",
    "    - but no need to do it on a per-row basis, if you have right stack.\n",
    "    - we can index multiple vectors per row.\n",
    "    - avoid repeating same metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823636de",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Other considerations\n",
    "\n",
    "- Combining GBDT (Gradient-Boosted Decision Trees) with neural features is quite effective.\n",
    "    - GBDT produces sparse vectors, one feature per leaf in the tree\n",
    "    - The feature is 1 if the input vector ends in that leaf."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695cb1ec",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## FAQ\n",
    "\n",
    "- What kind of metadata is useful to consider in RAG.\n",
    "    - Consider authoritative sources for this domain (e.g., doctors in health domain), rather than just \"drag up Reddit text\".\n",
    "    - Title and other metadata. It depends on use case.\n",
    "- Calibration of different indices\n",
    "    - different document indices are not aligned in terms of similarity scores\n",
    "    - have confidence scores for how likely is the recommendation to be good\n",
    "    - Answer: it is difficult. It might be a learning task, e.g., use GBDT for combining. \n",
    "        - But then you need to train the model with built training data. \n",
    "        - Can use LLM for generating this data.\n",
    "- Efficacy of re-rankers.\n",
    "    - They can help, but can make the response slower.\n",
    "- Combining similarity with interaction data\n",
    "    - It becomes a learning to rank problem. \n",
    "        - Type of interaction is used as label.\n",
    "        - You can use GBT where you can include the semantic score as well as a feature.\n",
    "- Jason Liu's blog on value of generating structured summaries and reports for decision makers instead of RAG\n",
    "- Future progress\n",
    "    - Use models with larger vocabularies, beyond BERT trained on 2018 data\n",
    "    - Not excited about using longer contexts because they provide lower precision.\n",
    "- Query search expansion, query understanding\n",
    "    - BM25 + reranker\n",
    "- Use case with lots of jargon and out-of-vocabulary words\n",
    "    - Need to use hybrid approach (keyword search + embedding), since embedding is poor at that.\n",
    "    - Sometimes we need to ignore the embedding altogether.\n",
    "- Colbert\n",
    "    - Yes with pretraining or longer vocabularies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4a5cfe",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Further reading\n",
    "\n",
    "- [Systematically improving RAG applications](https://www.youtube.com/watch?v=RrDBV6odPKo)\n",
    "- [Beyond the basics of Retrieval for Augmenting Generation](https://www.youtube.com/watch?v=0nA5QG3087g)\n",
    "- [Modern Information Retrieval Evaluation in the The RAG Era](https://www.youtube.com/watch?v=Trps2swgeOg)\n",
    "- [evaluating RAGs](https://jxnl.github.io/blog/writing/2024/02/28/levels-of-complexity-rag-applications/)\n",
    "- [6 RAG Evals](https://jxnl.co/writing/2025/05/19/there-are-only-6-rag-evals/)\n",
    "- [RAG is dead](https://pashpashpash.substack.com/p/why-i-no-longer-recommend-rag-for)\n",
    "- [RAG is not dead](https://hamel.dev/notes/llm/rag/not_dead.html)\n",
    "- [LangChain's retrieval](https://docs.langchain.com/oss/python/langchain/retrieval)\n",
    "- [OpenAI's retrieval](https://github.com/openai/openai-cookbook/blob/main/examples/Question_answering_using_embeddings.ipynb?ref=blog.langchain.com)\n",
    "- [RAG Theoretical concepts](https://www.youtube.com/watch?v=rhZgXNdhWDY)\n",
    "- [Building and Evaluating Advanced RAG Applications - DeepLearning.AI course](https://www.deeplearning.ai/short-courses/building-evaluating-advanced-rag/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e89428",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lg2",
   "language": "python",
   "name": "lg2"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
