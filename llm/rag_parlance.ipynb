{
 "cells": [
  {
   "cell_type": "raw",
   "id": "3198b7b7",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"RAG talks in Parlance\"\n",
    "author: \"Jaume Amores\"\n",
    "format:\n",
    "  revealjs:\n",
    "    scrollable: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfa603a",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Back to Basics for RAG\n",
    "\n",
    "[video](https://parlance-labs.com/education/rag/jo.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5551ede",
   "metadata": {},
   "source": [
    "## Demistifying RAG\n",
    "\n",
    "- It is basically \"stuffing text into the LLM prompt\".\n",
    "    - Typical use case: Q&A or search:\n",
    "        - Ask open-ended question.\n",
    "        - Retrieve content related to this question\n",
    "        - Use this content as context by stuffing it into the LLM prompt\n",
    "        - Result: the LLM response will be \"grounded\" in this context.\n",
    "        - It is not hallucination free but it might improve the accuracy of the generated answer.\n",
    "\n",
    "- Not necessarily related with Q&A or search.\n",
    "    - Example: labeller, retrieve positive and negative examples from dataset and present them in prompt to have the LLM label remaining following those examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ff070c",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "\n",
    "- Orchestration\n",
    "- Evaluation\n",
    "- Prompt\n",
    "- LLM\n",
    "- State (retrieval sources):\n",
    "    - File, Search Engine, Vector Database, Database, Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d18f290",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Evaluating Information Retrieval Systems\n",
    "\n",
    "- Query\n",
    "- Retrieval process\n",
    "- Result: ranked list of documents\n",
    "- Evaluation: relevance of ranking wrt query.\n",
    "    - Metrics: binary, non-binary\n",
    "    - Benchmarks: TREC, MS Marco, BEIR, ...\n",
    "    - Critical: build your own relevance dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5813f2f",
   "metadata": {},
   "source": [
    "## Relevance dataset\n",
    "\n",
    "- If we have real queries from production, spend time labelling results of those queries.\n",
    "- Otherwise, ask LLM to generate realistic synthetic queries.\n",
    "- Ideally static collection: no additions while evaluating and comparing, to keep consistency.\n",
    "- Using LLM judges to evaluate result.\n",
    "    - Need to find appropriate prompt to make it correlate with human judgement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b783c183",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Representational approaches\n",
    "\n",
    "- Avoid scoring all documents (e.g., if we use at web scale)\n",
    "- Have indexed documents, score only subset.\n",
    "- Approaches:\n",
    "    - Sparse, using inverted index.\n",
    "        - Top-k retrieval algorithms: WAND, ...\n",
    "        - Supervised (splade) or unsupervised (tf-idf)\n",
    "    - Dense: \n",
    "        - Vector index\n",
    "        - Accelerated search (approximate)\n",
    "        - Supervised via transf-learning (text embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0fdc59",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Dense representations\n",
    "\n",
    "- Encoder / Transformer style:\n",
    "    - Tokenize input text into discrete vocabulary.\n",
    "    - Pretrained learned representations of each token.\n",
    "    - Feed into encoder. For each token get an output vector.\n",
    "    - Pooling stage: average all vectors into a single vector representation. \n",
    "        - Weakest aspect. Diluted, low precision. Need shrinking mechanism?\n",
    "\n",
    "- Baseline / benchmark: BM25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4a5cfe",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Further reading\n",
    "\n",
    "- [Why fine-tuning is dead](https://www.youtube.com/watch?v=h1c_jmk97Ss)\n",
    "- [Systematically improving RAG applications](https://www.youtube.com/watch?v=RrDBV6odPKo)\n",
    "- [Beyond the basics of Retrieval for Augmenting Generation](https://www.youtube.com/watch?v=0nA5QG3087g)\n",
    "- [Modern Information Retrieval Evaluation in the The RAG Era](https://www.youtube.com/watch?v=Trps2swgeOg)\n",
    "- [Context Rot: When Long Context Fails](https://www.youtube.com/watch?v=3s_N60u0jEY)\n",
    "- [evaluating RAGs](https://jxnl.github.io/blog/writing/2024/02/28/levels-of-complexity-rag-applications/)\n",
    "- [6 RAG Evals](https://jxnl.co/writing/2025/05/19/there-are-only-6-rag-evals/)\n",
    "- [RAG is dead](https://pashpashpash.substack.com/p/why-i-no-longer-recommend-rag-for)\n",
    "- [RAG is not dead](https://hamel.dev/notes/llm/rag/not_dead.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e89428",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lg2",
   "language": "python",
   "name": "lg2"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
