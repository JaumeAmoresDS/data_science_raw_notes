{
 "cells": [
  {
   "cell_type": "raw",
   "id": "3198b7b7",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"Langchain courses\"\n",
    "author: \"Jaume Amores\"\n",
    "format:\n",
    "  revealjs:\n",
    "    scrollable: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7b1800",
   "metadata": {},
   "source": [
    "# State Reducers\n",
    "\n",
    "https://github.com/langchain-ai/langchain-academy/blob/main/module-2/state-reducers.ipynb\n",
    "\n",
    "## Custom Reducers\n",
    "\n",
    "def reduce_list(left: list | None, right: list | None) -> list:\n",
    "\n",
    "class CustomReducerState(TypedDict):\n",
    "    foo: Annotated[list[int], reduce_list]\n",
    "\n",
    "## Re-writing\n",
    "\n",
    "If we pass a message with the same ID as an existing one in our messages list, it will get overwritten!\n",
    "\n",
    "# New message to add\n",
    "new_message = HumanMessage(content=\"I'm looking for information on whales, specifically\", name=\"Lance\", id=\"2\")\n",
    "\n",
    "## Removal\n",
    "We can remove messages by using RemoveMessage.\n",
    "\n",
    "delete_messages = [RemoveMessage(id=m.id) for m in messages[:-2]]\n",
    "add_messages(messages , delete_messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33682551",
   "metadata": {},
   "source": [
    "# Multiple Schemas\n",
    "\n",
    "https://github.com/langchain-ai/langchain-academy/blob/main/module-2/multiple-schemas.ipynb\n",
    "\n",
    "## Private State\n",
    "\n",
    "class OverallState(TypedDict):\n",
    "    foo: int\n",
    "\n",
    "class PrivateState(TypedDict):\n",
    "    baz: int\n",
    "\n",
    "def node_1(state: OverallState) -> PrivateState:\n",
    "    print(\"---Node 1---\")\n",
    "    return {\"baz\": state['foo'] + 1}\n",
    "\n",
    "def node_2(state: PrivateState) -> OverallState:\n",
    "    print(\"---Node 2---\")\n",
    "    return {\"foo\": state['baz'] + 1}\n",
    "\n",
    "## Input / Output Schema\n",
    "\n",
    "class InputState(TypedDict):\n",
    "    question: str\n",
    "\n",
    "class OutputState(TypedDict):\n",
    "    answer: str\n",
    "\n",
    "class OverallState(TypedDict):\n",
    "    question: str\n",
    "    answer: str\n",
    "    notes: str\n",
    "\n",
    "def thinking_node(state: InputState):\n",
    "    return {\"answer\": \"bye\", \"notes\": \"... his is name is Lance\"}\n",
    "\n",
    "def answer_node(state: OverallState) -> OutputState:\n",
    "    return {\"answer\": \"bye Lance\"}\n",
    "\n",
    "graph = StateGraph(OverallState, input_schema=InputState, output_schema=OutputState)\n",
    "graph.add_node(\"answer_node\", answer_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7313a62",
   "metadata": {},
   "source": [
    "# Trim / Filter Messages\n",
    "\n",
    "https://github.com/langchain-ai/langchain-academy/blob/main/module-2/trim-filter-messages.ipynb\n",
    "\n",
    "## Reducer\n",
    "\n",
    "from langchain_core.messages import RemoveMessage\n",
    "\n",
    "def filter_messages(state: MessagesState):\n",
    "    # Delete all but the 2 most recent messages\n",
    "    delete_messages = [RemoveMessage(id=m.id) for m in state[\"messages\"][:-2]]\n",
    "    return {\"messages\": delete_messages}\n",
    "\n",
    "def chat_model_node(state: MessagesState):    \n",
    "    return {\"messages\": [llm.invoke(state[\"messages\"])]}\n",
    "\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"filter\", filter_messages)\n",
    "builder.add_node(\"chat_model\", chat_model_node)\n",
    "\n",
    "## Filtering messages\n",
    "\n",
    "def chat_model_node(state: MessagesState):\n",
    "    return {\"messages\": [llm.invoke(state[\"messages\"][-1:])]}\n",
    "\n",
    "## Trim messages\n",
    "\n",
    "from langchain_core.messages import trim_messages\n",
    "\n",
    "def chat_model_node(state: MessagesState):\n",
    "    messages = trim_messages(\n",
    "            state[\"messages\"],\n",
    "            max_tokens=100,\n",
    "            strategy=\"last\",\n",
    "            token_counter=ChatOpenAI(model=\"gpt-4o\"),\n",
    "            allow_partial=False,\n",
    "        )\n",
    "    return {\"messages\": [llm.invoke(messages)]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7c96a9",
   "metadata": {},
   "source": [
    "# Chatbot Summarization\n",
    "\n",
    "https://github.com/langchain-ai/langchain-academy/blob/main/module-2/chatbot-summarization.ipynb\n",
    "\n",
    "Building blocks:\n",
    "\n",
    "- RemoveMessage => remove all but the last 2 messages when there is a summary.\n",
    "- State with new field summary\n",
    "\n",
    "    class State(MessagesState):\n",
    "        summary: str\n",
    "\n",
    "- model node \n",
    "    - if summary exists, will append systemmessage asking to use it\n",
    "\n",
    "- summarize_conversation node\n",
    "    - will ask to summarize conversation, using existing summary if exists (by appending humanmenssage with summary embedded, asking to use the summary so far) or conversation up to now otherwise \n",
    "    - will output as new conversation (i.e, new state) the summary plus last 2 messages.\n",
    "        - this is done by using summary field and using RemoveMessage to remove all but last 2 messages in messages field.\n",
    "\n",
    "\n",
    "- should_continue for add_conditional_edges\n",
    "     - will move to summarize_conversation if more than X messages in conversation so far, or to END if converstaion still short.\n",
    "\n",
    "- memory: \n",
    "    - with config being configurable with thread_id, think about slack threads.\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b722f44",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Chatbot with External Memory\n",
    "\n",
    "https://github.com/langchain-ai/langchain-academy/blob/main/module-2/chatbot-external-memory.ipynb\n",
    "\n",
    "## SQLite\n",
    "\n",
    "import sqlite3\n",
    "db_path = \"state_db/example.db\"\n",
    "conn = sqlite3.connect(db_path, check_same_thread=False)\n",
    "\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "memory = SqliteSaver(conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e57de56",
   "metadata": {},
   "source": [
    "# UX and Human-in-the-Loop\n",
    "\n",
    "- Use cases:\n",
    "    - approval\n",
    "    - correction / editing / human taking action\n",
    "- State can be updated by human\n",
    "- Can be used for debugging\n",
    "- AI resume later\n",
    "\n",
    "# Streaming and Interruption\n",
    "\n",
    "https://github.com/langchain-ai/langchain-academy/blob/main/module-3/streaming-interruption.ipynb\n",
    "\n",
    "## Streaming modes\n",
    "\n",
    "- values, updates => full state, only updated part\n",
    "- tokens\n",
    "\n",
    "\n",
    "state-wise:\n",
    "\n",
    "```python\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "for chunk in graph.stream({\"messages\": [HumanMessage(content=\"hi! I'm Lance\")]}, config, stream_mode=\"updates\"):\n",
    "    print(chunk)\n",
    "```\n",
    "\n",
    "token-wise:\n",
    "- no stream_mode\n",
    "- async mode:\n",
    "\n",
    "```python\n",
    "async for event in graph.astream_events\n",
    "```\n",
    "\n",
    "- we print: event[\"metadata\"] (includes node in \"langraph_node\" field), event[\"event\"] (type), event[\"name\"]\n",
    "- we could also print event[\"data\"]\n",
    "- we can select to print only the llm calls from the node of the graph that we're interested in, in the example \"conversation\".\n",
    "   - we also indicate the event type, in our case \"on_chat_model_stream\" if we want to see llm responses\n",
    "- This provides chunk outputs, which are the tokens. We can print them nicely as \n",
    "\n",
    "```python\n",
    "if event[\"event\"]==\"on..\" and event[\"metadata\"][\"langgraph_node\"]==\"conversation\"\n",
    "data = event[\"data\"]\n",
    "print (data[\"chunk\"].content, end=\"|\")\n",
    "```\n",
    "\n",
    "### messages mode\n",
    "\n",
    "- assume `messages` field, which is list of messages.\n",
    "- \n",
    "\n",
    "## Langraph studio\n",
    "\n",
    "- We can take the client handle and retrieve:\n",
    "\n",
    "    - graphs running: assistants\n",
    "\n",
    "- Also create threads\n",
    "\n",
    "- Print data:\n",
    "\n",
    "this is the payload, and \"messages\" is the state\n",
    "\n",
    "```json\n",
    "{\n",
    "    'messages': [\n",
    "        {\n",
    "            'content': 'Multiply 2 and 3', \n",
    "            'additional_kwargs': {}, \n",
    "            'response_metadata': {}, \n",
    "            'type': 'human', \n",
    "            'name': None, \n",
    "            'id': '9aaa247f-1e6e-4451-af25-ac678fe46d82'\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb89a639",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e6ac68a9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e222c54c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Breakpoints\n",
    "\n",
    "https://github.com/langchain-ai/langchain-academy/blob/main/module-3/breakpoints.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc74faa",
   "metadata": {},
   "source": [
    "## Breakpoints with LangGraph API\n",
    "\n",
    "- We add `interrupt_before=[tools]` to the call `graph.compile(...)`\n",
    "\n",
    "```python\n",
    "graph = builder.compile(interrupt_before=[\"tools\"], checkpointer=memory)\n",
    "```\n",
    "\n",
    "- `x_ray` to `get_graph` => visualizes the breakpoints\n",
    "\n",
    "```python\n",
    "Image(graph.get_graph(xray=True).draw_mermaid_png())\n",
    "```\n",
    "\n",
    "- pass None when we invoke the graph => resume from previous checkpoint:\n",
    "\n",
    "```python\n",
    "for event in graph.stream(None, thread, stream_mode=\"values\"):\n",
    "```\n",
    "\n",
    "### Langgraph studio\n",
    "\n",
    "Equivalent calls:\n",
    "\n",
    "- First call for indicating interruption:\n",
    "\n",
    "```python\n",
    "thread = client.threads.create() # Difference 1: instead of creating thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "async for chunk in client.runs.stream ( # Difference 2: `async for` instead of `for`\n",
    "    thread[\"thread_id\"],\n",
    "    assistant_id=\"agent\", # Difference 3, we have several graphs and need to indicate which one\n",
    "    input=initial_input, # if placed as first argument, no need to use keyword input\n",
    "    stream_mode=\"values\", # same\n",
    "    interrupt_before=[\"tools\"], # Difference 4: breakpoint indicating in stream command \n",
    "                                # instead of compile (since the graph has already been compiled)\n",
    ")\n",
    "```\n",
    "\n",
    "- Second call for resuming:\n",
    "\n",
    "```python\n",
    "async for chunk in client.runs.stream ( \n",
    "    thread[\"thread_id\"],\n",
    "    assistant_id=\"agent\", \n",
    "    input=None, # None goes here\n",
    "    stream_mode=\"values\", \n",
    "    interrupt_before=[\"tools\"],\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35dfc467",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Editing state and human feedback\n",
    "\n",
    "https://github.com/langchain-ai/langchain-academy/blob/main/module-3/edit-state-human-feedback.ipynb\n",
    "\n",
    "## Update state / message\n",
    "\n",
    "\n",
    "- First option: append message to previous list (add_messages reducer)\n",
    "\n",
    "```python\n",
    "graph.update_state (\n",
    "    thread,\n",
    "    {\"messages\": [HumanMessage(content=\"my new message\")]}\n",
    ")\n",
    "```\n",
    "\n",
    "- Second option: replace the message. We need to supply the message id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2244b7df",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "```python\n",
    "state = graph.get_state(thread)\n",
    "```\n",
    "\n",
    "or \n",
    "\n",
    "```python\n",
    "state = await client.threads.get_state(thread[\"thread_id\"])\n",
    "```\n",
    "\n",
    "and then:\n",
    "\n",
    "```python\n",
    "new_message = state[\"values\"][\"messages\"][-1]\n",
    "new_message[\"content\"] = \"my new message\"\n",
    "new_state = {\"messages\": new_message}\n",
    "```\n",
    "\n",
    "and:\n",
    "\n",
    "```python\n",
    "graph.update_state(thread, new_state)\n",
    "```\n",
    "\n",
    "or:\n",
    "\n",
    "```python\n",
    "await client.threads.update_state(thread[\"thread_id\"], new_state)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71e9d00",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Langgraph studio\n",
    "\n",
    "- We can add interruptions through the UI, besides the node icon in the graph\n",
    "- We can edit the messages in the chat. Afterwards, we need to **fork** the thread by pressing the button \"fork\" below the message.\n",
    "- After editing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdc387c",
   "metadata": {},
   "source": [
    "## Awaiting user input\n",
    "\n",
    "- add placeholder (no-op) function for collecting human feedback:\n",
    "\n",
    "```python\n",
    "def human_feedback (state: MessageState):\n",
    "    pass\n",
    "\n",
    "builder.add_node (\"human_feedback\", human_feedback)\n",
    "```\n",
    "\n",
    "- add interruption before human feedback node.\n",
    "\n",
    "```python\n",
    "graph = builder.compile (..., interrupt_before=[\"human_feedback\"])\n",
    "```\n",
    "\n",
    "- when updating state, add `as_node=\"human_feedback\"` to imitate the case that the message was updated in that node.\n",
    "\n",
    "```python\n",
    "new_message = input (\"new message: \")\n",
    "graph.update_state (thread, new_message, as_node=\"human_feedback\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93281ec7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Dynamic breakpoints\n",
    "\n",
    "[notebook](https://github.com/langchain-ai/langchain-academy/blob/main/module-3/dynamic-breakpoints.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c9847d",
   "metadata": {},
   "source": [
    "## Conditional\n",
    "\n",
    "- Benefits: \n",
    "    - triggered only on certain conditions.\n",
    "    - allows to communicate reason.\n",
    "\n",
    "\n",
    "- In node, if condition occurs, a `NodeInterrupt` error is raised:\n",
    "\n",
    "```python\n",
    "from langgraph.errors import NodeInterrupt\n",
    "\n",
    "def node (state: MyStateClass):\n",
    "    if condition:\n",
    "        raise NodeInterrupt (f\"State {state} meets condition\")\n",
    "```\n",
    "\n",
    "We can see breakpoint information as:\n",
    "\n",
    "```python\n",
    "state = graph.get_state (thread_config)\n",
    "print (state.tasks)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64eb52b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Time Travel\n",
    "\n",
    "[notebook](https://github.com/langchain-ai/langchain-academy/blob/main/module-3/time-travel.ipynb)\n",
    "\n",
    "## Replay\n",
    "\n",
    "- Get history with:\n",
    "\n",
    "```python\n",
    "state_history = [s for s in graph.get_state_history(thread)]\n",
    "```\n",
    "\n",
    "- Replay using:\n",
    "\n",
    "```python\n",
    "for event in graph.stream (input=None, state_history[idx].config, stream_mode=\"values\"):\n",
    "    ... \n",
    "```\n",
    "\n",
    "![checkpoint_history.jpg](time_travel.png)\n",
    "\n",
    "## Fork\n",
    "\n",
    "- Used to replay with different state values.\n",
    "\n",
    "\n",
    "```python\n",
    "new_state = ...\n",
    "fork_config = graph.update_state (state_history[idx].config, new_state)\n",
    "for event in graph.stream (None, fork_config, stream_mode=\"values\"):\n",
    "    ...\n",
    "```\n",
    "\n",
    "In case we use `MessageState`, we'll want to overwrite the previous message with a new one, by passing the message ID:\n",
    "\n",
    "```python\n",
    "new_state = {\"messages\": [HumanMessage(content=\"new message\", id=state_history[idx].values[\"messages\"][0].id)]}\n",
    "```\n",
    "\n",
    "### Using Langgraph studio\n",
    "\n",
    "```python\n",
    "state_history = client.threads.get_history (thread[\"thread_id\"])\n",
    "forked_input = {\"messages\": [HumanMessage(content=\"my new message\", id=state_histor[idx][\"values\"][\"messages\"][0][\"id\"]]}\n",
    "await client.threads.update_state (\n",
    "    thread[\"thread_id\"],\n",
    "    forked_input,\n",
    "    checkpoint_id=state_history[idx][\"checkpoint_id\"]\n",
    ")\n",
    "async for chunk in client.runs.stream (\n",
    "    thread[\"thread_id\"],\n",
    "    assistant_id=\"agent\",\n",
    "    input=None,\n",
    "    stream_updates=\"updates\",\n",
    "    checkpoint_id=state_history[idx][\"checkpoint_id\"]\n",
    ")\n",
    "```\n",
    "\n",
    "### Notes\n",
    "\n",
    "When we udpate the state we are actually adding a new checkpoint to the history, so it becomes larger and larger."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8e6272",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Building your assistant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722ba0bc",
   "metadata": {},
   "source": [
    "# Parallelization\n",
    "\n",
    "[notebook](https://github.com/langchain-ai/langchain-academy/blob/main/module-4/parallelization.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06102f3a",
   "metadata": {},
   "source": [
    "## Control execution order of parallel nodes\n",
    "\n",
    "Several ways:\n",
    "\n",
    "1. Reducer \n",
    "`\n",
    "```python\n",
    "def sorted_reducer (left, right):\n",
    "    if not isinstance(left, list):\n",
    "        left = [left]\n",
    "    if not isinstance(right, list):\n",
    "        right = [right]\n",
    "\n",
    "    return sorted (left+right)\n",
    "```\n",
    "\n",
    "2. Sink node:\n",
    "\n",
    "- Write updates to different fields of the state\n",
    "- Sink node joins the updates in whichever way considers appropriate\n",
    "- Then it deletes the temporary fields from state.\n",
    "\n",
    "## Realistic example\n",
    "\n",
    "- Query web and wikipedia to get context, and have LLM use it to respond query.\n",
    "- Web search: tavily, with API key. Import from `langchain_tavily`\n",
    "- Wikipedia search: WikipediaLoader from `langchain_community.document_loaders`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f905977",
   "metadata": {},
   "source": [
    "# Sub-graphs\n",
    "\n",
    "- Sub-graphs can be used for different agents in a multi-agent graph.\n",
    "- A sub-graph is a node where we pass a compiled graph as \"function\".\n",
    "- Each sub-graph can have a different input and output state schema. \n",
    "\n",
    "\n",
    "```python\n",
    "fa_builder = StateGraph (state_schema=FailureAnalysisState, output_schema=FailureAnalysisOutputState)\n",
    "qs_builder = StateGraph(QuestionSummarizationState,output_schema=QuestionSummarizationOutputState)\n",
    "\n",
    "entry_builder.add_node (\"failure_analysis\", fa_builder.compile())\n",
    "entry_builder.add_node (\"qs_analysis\", qs_builder.compile())\n",
    "```\n",
    "\n",
    "The state introduce to failure_analysis and qs_analysis sub-graphs can have additional fields that are not present in the schema of those sub-graphs:\n",
    "\n",
    "```python\n",
    "class EntryGraphState(TypedDict):\n",
    "    raw_logs: List[Log]             # not present in sub-graphs schemas: internal field\n",
    "    cleaned_logs: List[Log]         # overlap: input used by sub-graphs\n",
    "    fa_summary: str                 # overlap with fa schema: output provided by fa subgraph\n",
    "    report: str                     # overlap with qs schema: output provided by qs subgraph\n",
    "    processed_logs:  Annotated[List[int], add] # overlap with both schemas: output provided by both sub-graphs (has reducer)\n",
    "\n",
    "class FailureAnalysisState(TypedDict):\n",
    "    cleaned_logs: List[Log]         # overlap\n",
    "    failures: List[Log]             # new: internal field\n",
    "    fa_summary: str                 # overlap\n",
    "    processed_logs: List[str]       # overlap\n",
    "\n",
    "class QuestionSummarizationState(TypedDict):\n",
    "    cleaned_logs: List[Log]         # overlap \n",
    "    qs_summary: str                 # new: internal field\n",
    "    report: str                     # overlap\n",
    "    processed_logs: List[str]       # overlap\n",
    "```\n",
    "\n",
    "A node can return a state with only some of the fields of its output state schema.\n",
    "\n",
    "```python\n",
    "def send_to_slack(state):\n",
    "    qs_summary = state[\"qs_summary\"]\n",
    "    # Add fxn: report = report_generation(qs_summary)\n",
    "    report = \"foo bar baz\"\n",
    "    return {\"report\": report}\n",
    "```\n",
    "\n",
    "### Notes\n",
    "\n",
    "If the sub-graphs don't use a different output schema, then we need to indicate reducers for fields such as `cleaned_logs` that, although unmodified, are used by both sub-graphs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fee9c4",
   "metadata": {},
   "source": [
    "# Map-Reduce\n",
    "\n",
    "[notebook](https://github.com/langchain-ai/langchain-academy/blob/main/module-4/map-reduce.ipynb)\n",
    "\n",
    "## Using send and conditional edges\n",
    "\n",
    "- To go from a single node `before_map` to copies of a node that run in parallel (`map` step), define a function that uses `Send` to the node:\n",
    "\n",
    "```python\n",
    "def my_map (state):\n",
    "    return [Send(\"my_map_node\", {\"my_node_input\": s}) for s in state[\"my_list_of_inputs\"]] \n",
    "```\n",
    "\n",
    "Then add a conditional edge from the single node `before_map` to the parallel list of clones of `my_map` node:\n",
    "\n",
    "```python\n",
    "builder.add_conditional_edges (\"before_map\", \"my_map\", [\"my_map_node\"])\n",
    "```\n",
    "\n",
    "And add a normal edge that goes from `my_map_node` to `my_reducer_node`:\n",
    "\n",
    "```python\n",
    "builder.add_edge (\"my_map_node\", \"my_reducer_node\")\n",
    "```\n",
    "\n",
    "- `my_map_node` needs to output to a field which has a reducer function in it:\n",
    "\n",
    "```python\n",
    "class MyOverallState ():\n",
    "    my_output_field: Annotated[my_output_type, my_reducer_function]\n",
    "\n",
    "def my_map_node (state: MyOverallState):\n",
    "    ...\n",
    "    return {\"my_output_field\": my_return_value} \n",
    "``` \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b4ea76",
   "metadata": {},
   "source": [
    "## Notes about example used in tutorial\n",
    "\n",
    "- We use structured output for the LLM:\n",
    "\n",
    "```python\n",
    "model = ChatOpenAI (...)\n",
    "\n",
    "def my_node ():\n",
    "    response = model.with_structured_output(MyStateClass).invoke(my_prompt)\n",
    "    return {\"my_output_field\": response.my_field}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9ceb23",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Implementation details\n",
    "\n",
    "We can get access to the model response content as `response.content`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feaf7e43",
   "metadata": {},
   "source": [
    "# Long term memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a298d0be",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "[slides](https://files.cdn.thinkific.com/file_uploads/967498/attachments/dc4/f52/87a/LangChain_Academy_-_Introduction_to_LangGraph_-_Long-Term_Memory.pdf)\n",
    "[tutorial](https://docs.langchain.com/oss/python/langgraph/memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2461480c",
   "metadata": {},
   "source": [
    "## Short vs Long term memory\n",
    "\n",
    "- Short term memory: \n",
    "    - in session, single thread\n",
    "    - with checkpointer\n",
    "    - past messages can be summarized or filtered (e.g., truncate them)\n",
    "- Long term memory: \n",
    "    - across sessions, across threads\n",
    "    - with store\n",
    "\n",
    "## Long term memory:\n",
    "\n",
    "### Type of memory:\n",
    "\n",
    "#### 1 Semantic \n",
    "\n",
    "- facts, user data\n",
    "- structure: profile (dict with fields) or list of items (e.g., locations), updated after each session\n",
    "- Pros:\n",
    "    - Single document (profile): easily retrieved\n",
    "    - List: narrow scope, easy to add\n",
    "- Cons:\n",
    "    - Single document: difficult to maintain when it grows\n",
    "    - List: costly to retrieve as it grows.\n",
    "\n",
    "#### 2 Episodic\n",
    "\n",
    "- memories: agent actions\n",
    "\n",
    "#### 3 Procedural\n",
    "\n",
    "- prompts\n",
    "- Using AI to generate prompts based on human feedback, tests and evaluation scores (LangSmith)\n",
    "\n",
    "[video](https://www.youtube.com/watch?v=Vn8A3BxfplE)\n",
    "[notebook](https://github.com/langchain-ai/langsmith-cookbook/blob/main/optimization/assisted-prompt-bootstrapping/elvis-bot.ipynb)\n",
    "\n",
    "### Updates\n",
    "\n",
    "#### 1 Hot path\n",
    "\n",
    "Pro: Real-time and transparent\n",
    "Con: delays / bad UX\n",
    "[github code](https://github.com/langchain-ai/memory-agent)\n",
    "\n",
    "#### 2 Background \n",
    "\n",
    "Pro: no delays / good UX\n",
    "Con: Frequency of writing to be tuned\n",
    "[github code](https://github.com/langchain-ai/memory-template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cd31ab",
   "metadata": {},
   "source": [
    "# LangGraph Store\n",
    "\n",
    "[notebook](https://github.com/langchain-ai/langchain-academy/blob/main/module-5/memory_store.ipynb)\n",
    "\n",
    "## Implementation details\n",
    "\n",
    "```python\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "```\n",
    "\n",
    "- **Memory saved** using:\n",
    "    - namespace: tuple, like directory\n",
    "    ```python\n",
    "    namespace = (user_id, \"memories\")\n",
    "    ```\n",
    "    - key: string, like filename\n",
    "    ```python\n",
    "    key = \"user_memory\"\n",
    "    ```\n",
    "    - value: like content of file: \n",
    "    ```python\n",
    "    value = {\"food_preference\" : \"I like pizza\"}\n",
    "    ```\n",
    "- Write: **put**\n",
    "```python\n",
    "    store.put (namespace, key, value)\n",
    "```\n",
    "- Read: **get**\n",
    "```python\n",
    "    memory = store.get (namespace, key)\n",
    "```\n",
    "- **Retrieve all**:\n",
    "```python\n",
    "    memories = store.search (namespace)\n",
    "```\n",
    "- **config passed**: \n",
    "\n",
    "```python\n",
    "{\"configurable\": {\"thread_id\": thread_id, \"my_key_info\": my_key_info}}\n",
    "```\n",
    "\n",
    "- **Important:** The store needs to be passed to the node so that it can explicitly read values from the history and act\n",
    "on them, e.g., summarizing the memories or considering them in some specific manner:\n",
    "\n",
    "```python\n",
    "def my_node_function (state: MyStateClass, config: RunnableConfig, store: BaseStore):\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "```\n",
    "\n",
    "- compiling with both short-term and long-term memory:\n",
    "\n",
    "```python\n",
    "builder.compile (\n",
    "    checkpointer=my_checkpointer, # short_term_memory\n",
    "    store=my_store, # long_term_memory\n",
    ")\n",
    "```\n",
    "\n",
    "```python\n",
    "config = {\"configurable\": {\"thread_id\": \"1\", \"user_id\": \"1\"}}\n",
    "for event in graph.stream (my_messages, config, stream_mode=\"values\"):\n",
    "    ...\n",
    "\n",
    "# we have across-session memory, so we can pass another thread_id:\n",
    "config = {\"configurable\": {\"thread_id\": \"2\", \"user_id\": \"1\"}}\n",
    "for event in graph.stream (my_messages, config, stream_mode=\"values\"):\n",
    "    ...\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4a15f8",
   "metadata": {},
   "source": [
    "# Memory schema + profile\n",
    "\n",
    "[notebook](https://github.com/langchain-ai/langchain-academy/blob/main/module-5/memoryschema_profile.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8377ea68",
   "metadata": {},
   "source": [
    "## Model with structured output \n",
    "\n",
    "- Use `with_structured_output` method to adhere to specific profile fields\n",
    "\n",
    "```python\n",
    "new_memory = model.with_structured_output (MyProfileSchema)\n",
    "new_memory = my_format.format(new_memory)\n",
    "```\n",
    "\n",
    "## TrustCall\n",
    "\n",
    "- Adhere to more complex schemas (e.g., based on pydantic and multiple classes)\n",
    "- Update complex schemas without having to regenerate the whole schema and overwrite (i.e., more efficiently)\n",
    "\n",
    "### Call\n",
    "\n",
    "We pass:\n",
    "\n",
    "- A model\n",
    "- A schema as a tool: `tools = [MySchema]`\n",
    "- A tool choice name for enforcing output to respect this schema: `tool_choise = \"MySchema\"`\n",
    "\n",
    "We retrieve:\n",
    "\n",
    "- AI messages: \n",
    "```python\n",
    "result[\"messages\"]\n",
    "```\n",
    "- Structured output: \n",
    "```python\n",
    "result[\"responses\"] # list of MySchema objects \n",
    "result[\"responses\"][0].model_dump()\n",
    "```\n",
    "- Metadata: result[\"response_metadata\"]\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "from trustcall import create_extractor\n",
    "from pydantic import BaseModel, Field, ValidationError\n",
    "\n",
    "class MySchema (BaseModel):\n",
    "    name: str = Field (description=\"user name\")\n",
    "    interests: List[str] = Field (description=\"list of user interests\")\n",
    "\n",
    "conversation = [\n",
    "    HumanMessage (content=\"Hi I'm Jaume\"),\n",
    "    AIMessage (content=\"Hi Jaume, how can I assist you?\")\n",
    "    HumanMessage (content=\"I like biking for cardio and sightseeing\")\n",
    "]\n",
    "\n",
    "extractor = create_extractor (\n",
    "    model=model,\n",
    "    tools=[MySchema],\n",
    "    tool_choice=\"MySchema\",\n",
    ")\n",
    "\n",
    "system_message=SystemMessage(content=\"Extract user details from this conversation\")\n",
    "extractor.invoke ({\"messages\": [system_message] + conversation})\n",
    "```\n",
    "\n",
    "## Update \n",
    "\n",
    "- Produce a json patch\n",
    "- We pass the serialized object using \n",
    "```python\n",
    "{\"existing\": {\"MySchema\": result[\"response\"][0].model_dump()}}\n",
    "```\n",
    "\n",
    "Note that we indicate the name of the class to respect, \"MySchema\"\n",
    "\n",
    "Full call:\n",
    "```python\n",
    "creator.invoke (\n",
    "    {\"messages\": [system_message] + updated_conversation}, \n",
    "    {\"existing\": {\"MySchema\": result[\"response\"][0].model_dump()}}\n",
    ")\n",
    "```\n",
    "\n",
    "We can also put \"messages\" and \"existing\" into a single dictionary:\n",
    "```python\n",
    "{\"messages\": [...], \"existing\": existing_memory_as_dict}\n",
    "```\n",
    "\n",
    "When writing into our store, we need to deserialize the object:\n",
    "\n",
    "```python\n",
    "my_store.put (namespace, key, result[\"responses\"][0].model_dump())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5203f56",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Memory Schema + Collection\n",
    "\n",
    "[notebook](https://github.com/langchain-ai/langchain-academy/blob/main/module-5/memoryschema_collection.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3f7d3f",
   "metadata": {},
   "source": [
    "## model with structured output\n",
    "\n",
    "- Define using BaseModel\n",
    "\n",
    "```python\n",
    "class MyMemory (BaseModel):\n",
    "    memory: str = Field (description=\"One of the memories\")\n",
    "\n",
    "class MyCollection (BaseModel):\n",
    "    my_collection: List[MyMemory] = Field (description=\"\")\n",
    "\n",
    "model_with_structure = mode.with_structured_output (MyCollection)\n",
    "result = model_with_structure.invoke (...)\n",
    "```\n",
    "\n",
    "- Save to store using put with one key per item in the collection:\n",
    "\n",
    "```python\n",
    "for item in result.my_collection:\n",
    "    key = str(uuid.uuid4 ())\n",
    "    my_store.put (namespace, key, item.model_dump())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9c4acd",
   "metadata": {},
   "source": [
    "## TrustCall\n",
    "\n",
    "- use single element class (`MyMemory` in previous example)\n",
    "- pass `enable_inserts=True` when building the extractor object\n",
    "\n",
    "```python\n",
    "extractor = create_extractor (\n",
    "    model,\n",
    "    tools=[MyMemory],\n",
    "    tool_choice=[\"MyMemory\"],\n",
    "    enable_inserts=True,\n",
    ")\n",
    "\n",
    "system_msg = \"\"\"Update existing memories and create new ones based on the following conversation:\"\"\"\n",
    "\n",
    "extractor.invoke({\"messages\": [SystemMessage(content=system_msg)]+updated_conversation, \"existing\": existing_memories})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34088065",
   "metadata": {},
   "source": [
    "## Graph + TrustCall\n",
    "\n",
    "- TrustCall instruction prompt: \"... use parallel tool calling to handle updates and insertions simulatenously:\"\n",
    "\n",
    "- when using memories to adapt the assistant response: since it is a list, we format the {memory} section of the prompt using the list of memories so far retrieved with search:\n",
    "\n",
    "```python\n",
    "memories = my_store.search(namespace)\n",
    "formatted_memories = \"\\n\".join([mem.value[\"content\"] for mem in memories])\n",
    "```\n",
    "\n",
    "- existing_memories construction:\n",
    "```python\n",
    "namespace=(\"memory\", user_id)\n",
    "existing_items = my_store.search(namespace)\n",
    "tool_name=\"Memory\"\n",
    "existing_memories = ([(existing_item.key, tool_name, existing_item.value) for existing_item in existing_items] if existing_items else None)\n",
    "\n",
    "```\n",
    "- using `merge_message_runs`\n",
    "```python\n",
    "system_msg = SystemMessage(content=\"my prompt message\")\n",
    "updated_messages = list(merge_message_runs(messages=[system_msg]+state[\"messages\"]))\n",
    "result=creator.invoke ({\"messages\": updated_messages, \"existing\": existing_memories})\n",
    "```\n",
    "- store update\n",
    "```python\n",
    "for resp, rmeta in zip(result[\"responses\"], result[\"response_metadata\"]):\n",
    "    my_store.put (\n",
    "        namespace, \n",
    "        rmeta.get(\"json_doc_id\", uuid.uuid4()), # if memory is new, it is appended (add_messages reducer) since it has a new ID. If it is an existing one, it is overwritten, since we provide its previous json_doc_id as ID\n",
    "        resp.model_dump(mode=\"json\")\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1ab74f",
   "metadata": {},
   "source": [
    "# Memory agent\n",
    "\n",
    "[notebook](https://github.com/langchain-ai/langchain-academy/blob/main/module-5/memory_agent.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8136f4cc",
   "metadata": {},
   "source": [
    "## Listener in TrustCall\n",
    "\n",
    "Log tool calls done by TrustCall\n",
    "\n",
    "Surface things like:\n",
    "- actions to solve schema validation errors, and \n",
    "- updates done to previous memories\n",
    "\n",
    "```python\n",
    "class Spy:\n",
    "    ...\n",
    "spy = Spy()\n",
    "extractor = create_extractor(...)\n",
    "extractor_with_listener = extractor.with_listeners(on_end=spy)\n",
    "\n",
    "\n",
    "class Spy:\n",
    "    def __init__ (self):\n",
    "        self.called_tools = []\n",
    "    \n",
    "    def __call__ (self, run):\n",
    "        q = [run]\n",
    "        while q:\n",
    "            r = q.pop()\n",
    "            if r.child_runs:\n",
    "                q.extend(r.child_runs)\n",
    "            if r.run_type==\"chat_model\":\n",
    "                self.called_tools.append(r.outputs[\"generations\"][0][0][\"messages\"][\"kwargs\"][\"tool_calls\"])\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f5cc30",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Implementation details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53128676",
   "metadata": {},
   "source": [
    "When calling a tool node it is very important that the node responds back notifying that the call was made:\n",
    "\n",
    "```python\n",
    "def tool_node (...):\n",
    "    # ...\n",
    "    tool_calls = state[\"messages\"][-1].tool_calls\n",
    "    return {\"messages\": [{\"role\": \"tool\", \"content\": \"my tool response\", \"tool_call_id\": tool_calls[0][\"id\"]}]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde8bfdf",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eeb670e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Deployment concepts\n",
    "\n",
    "[notebook](https://files.cdn.thinkific.com/file_uploads/967498/attachments/5d8/68e/5fd/LangChain_Academy_-_Introduction_to_LangGraph_-_Deployment.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b74382",
   "metadata": {},
   "source": [
    "# creating\n",
    "\n",
    "[notebook](https://github.com/langchain-ai/langchain-academy/blob/main/module-6/creating.ipynb)\n",
    "\n",
    "- langgraph cli, docker and docker compose\n",
    "\n",
    "# connecting\n",
    "\n",
    "## client / remote graph\n",
    "\n",
    "```python\n",
    "\n",
    "from langgraph_sdk import get_client\n",
    "\n",
    "\n",
    "# langgraph-api:\n",
    "#         image: \"lg_image\"\n",
    "#         ports:\n",
    "#             - \"8123:8000\"\n",
    "\n",
    "url = \"http://localhost:8123\" # \n",
    "client = get_client (url=url)\n",
    "\n",
    "# or ...\n",
    "remote_graph = RemoteGraph (graph_name, url=url)\n",
    "# (how to use remote_graph?)\n",
    "\n",
    "```\n",
    "\n",
    "## managing runs\n",
    "\n",
    "- list: `client.runs.list(thread[\"thread_id\"])`\n",
    "- create thread: `client.threads.create()`\n",
    "- create run: \n",
    "    ```python\n",
    "    client.runs.create(thread[\"thread_id\"], graph_name, input=input_message, config=config)\n",
    "    ```\n",
    "    - Note: `config` here only contains the `user_id`, not the `thread_id`, since that piece is passed in the first argument.\n",
    "- get run status: `client.runs.get(...)`\n",
    "- block until complete (join): `client.runs.join(...)`\n",
    "- stream (e.g., tokens): `client.runs.stream(...)`: same as create run, but adding parameter `stream_mode=\"messages-tuple\"`\n",
    "\n",
    "## threads\n",
    "\n",
    "- For working with multi-turn interactions, with multiple graphs executions for a given thread.\n",
    "- The server stores the checkpoints of the thread in Postgres.\n",
    "- We can:\n",
    "    - get state checkpoints saved: \n",
    "\n",
    "    ```python\n",
    "    thread_state = await client.threads.get_state(thread[\"thread_id\"])\n",
    "    for m in convert_to_messages (thread_state[\"values\"][\"messages\"]):\n",
    "        m.pretty_print()\n",
    "    ```\n",
    "\n",
    "    - copy (fork) thread: `client.threads.copy(thread[\"thread_id\"])`\n",
    "    - do human-in-the-loop:\n",
    "\n",
    "    ```python\n",
    "    states = await client.threads.get_history(thread[\"thread_id\"])\n",
    "    to_fork = states[-2]\n",
    "    # to_fork[\"values\"], to_fork[\"next\"]\n",
    "    message_id = to_fork[\"values\"][\"messages\"][0][\"id\"]\n",
    "    checkpoint_id = to_fork[\"checkpoint_id\"]\n",
    "    forked_input = {\n",
    "        \"messages\": [HumanMessage(content=\"my new message\", id=message_id)] # overwrite previuos message by supplying ID\n",
    "    }\n",
    "    forked_config = await.client.threads.update_state(\n",
    "        thread[\"thread_id\"],\n",
    "        forked_input,\n",
    "        checkpoint_id=checkpoint_id\n",
    "    )\n",
    "    ```\n",
    "\n",
    "    - stream using updated input from new checkpoint\n",
    "        - same call as last stream but with:\n",
    "            - input=None, so that it resumes\n",
    "            - adding `checkpoint_id=checkpoint_id`\n",
    "\n",
    "## across-thread memory\n",
    "\n",
    "The memory store is stored in Postgres\n",
    "\n",
    "We can:\n",
    "\n",
    "- search items by namespace: `client.store.search(namespace)` where namspace is a tuple\n",
    "- put new items: `client.store.put_item (namespace, key=key, value=value)`\n",
    "- delete items: `client.store.delete_item(namespace, key=key)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8dce41d",
   "metadata": {},
   "source": [
    "# Double-texting\n",
    "\n",
    "[notebook](https://github.com/langchain-ai/langchain-academy/blob/main/module-6/double-texting.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ac4464",
   "metadata": {},
   "source": [
    "## Reject\n",
    "\n",
    "- runs.create with input_1\n",
    "- try:\n",
    "    - runs.create with input_2\n",
    "- except httpx.HTTPStatusError as e:\n",
    "...\n",
    "\n",
    "## Enqueue\n",
    "\n",
    "- To new runs we add the argument `multitask_strategy=\"enqueue\"`\n",
    "```python\n",
    "second_run = await client.runs.create(..., multitask_strategy=\"enqueue\")\n",
    "```\n",
    "- We await for the last run: `await client.runs.join(thread[\"thread_id\"], second_run[\"run_id\"])`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8eec925",
   "metadata": {},
   "source": [
    "## Interrupt\n",
    "\n",
    "- The previous message will be interrupted and the new one take over.\n",
    "- How: as before, but we pass the argument `multitask_strategy=\"interrupt\"`. Again, we await for the last run to complete.\n",
    "- The final status is \"interrupted\".\n",
    "\n",
    "## Rollback\n",
    "\n",
    "- Instead of keeping the interrupted messages, a new run is created, the old one is deleted, and the new run takes only the new message.\n",
    "- How, as before, but we pass the argument `multitask_strategy=\"rollback\"`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ea8c5d",
   "metadata": {},
   "source": [
    "# Assistants\n",
    "\n",
    "[notebook](https://github.com/langchain-ai/langchain-academy/blob/main/module-6/assistant.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b335178",
   "metadata": {},
   "source": [
    "## Creating assistants\n",
    "\n",
    "```python\n",
    "personal_assistant = await client.assistants.create(graph_name, config={\"configurable\": {\"todo_category\": \"personal\"}})\n",
    "```\n",
    "\n",
    "## Updating assistants\n",
    "\n",
    "- When updading an assistant, we are creating a new version of it.\n",
    "\n",
    "```python\n",
    "configurations = {\n",
    "    \"todo_category\": \"personal\",\n",
    "    \"user_id\": \"lance\",\n",
    "    \"task_maistro_role\": new_prompt,\n",
    "}\n",
    "personal_assistant = await client.assistants.update(\n",
    "    personal_assistant[\"assistant_id\"],\n",
    "    config={\"configurable\": configurations},\n",
    ")\n",
    "```\n",
    "\n",
    "- Notes:\n",
    "    - The fields in the configurations dict coincide with the attributes of the `Configurations` class defined in deployment/configuration.py\n",
    "    - Inside the llm node of the graph, in the `task_maistro.py` file, we have an argument of type `RunnableConfig`, which gives us a dictionary. This dictionary is transformed into a Configurations object using the classmethod `from_runnable_config`:\n",
    "\n",
    "```python\n",
    "def task_mAIstro (state: MessagesState, config: RunnableConfig, store: BaseStore):\n",
    "    configurable = configuration.Configuration.from_runnable_config(config)\n",
    "    # configurable.todo_category, configurable.user_id, configurable.task_maistro_role\n",
    "```\n",
    "        \n",
    "\n",
    "## Managing assistants\n",
    "\n",
    "- List assistants:\n",
    "\n",
    "```python\n",
    "assistants = await client.assistants.search()\n",
    "\n",
    "for assistant in assistants:\n",
    "    print ({\n",
    "        \"assistant_id\": assistant[\"assistant_id\"],\n",
    "        \"version\": assistant[\"version\"],\n",
    "        \"config\": assistant[\"config\"], \n",
    "    })\n",
    "\n",
    "# config includes category, user_id, and role, as supplied in configurations dict above\n",
    "```\n",
    "\n",
    "- Delete assistant:\n",
    "\n",
    "```python\n",
    "await client.assistants.delete(assistant[\"assistant_id\"])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7cf72c",
   "metadata": {},
   "source": [
    "## Using assistants\n",
    "\n",
    "```python\n",
    "thread = await client.threads.create()\n",
    "async for chunk in client.runs.stream(\n",
    "    thread[\"thread_id\"],\n",
    "    assistant_id,\n",
    "    input={\"messages\": [HumanMessage(content=\"my message\")]},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    if chunk.event == \"values\":\n",
    "        state = chunk.data\n",
    "        convert_to_messages(state[\"messages\"])[-1].pretty_print()\n",
    "```\n",
    "\n",
    "- Notes:\n",
    "    - when using `graph.stream`, we get an `event` dict where `event[\"messages]` is a list of Message objects (e.g., HumanMessage, AIMessage, etc.) on which we can call `pretty_print`, i.e., `event[\"messages\"][-1].pretty_print()`\n",
    "    - when using `client.runs.stream`, we get a `chunk` object, and the data is in `chunk.data`. Furthermore, `chunk.data[\"messages\"]` is a list of dicts on which cannot call `pretty_print()` directly, so we need to convert them first using `convert_to_messages`.\n",
    "\n",
    "```python\n",
    "for event in graph.stream(None, to_replay.config, stream_mode=\"values\"):\n",
    "    event['messages'][-1].pretty_print()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4c9a30",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lg2",
   "language": "python",
   "name": "lg2"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
